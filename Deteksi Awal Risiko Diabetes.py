# -*- coding: utf-8 -*-
"""Salinan dari pima indians diabetes database.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19_5gz14kzSfvJYoUnaUxCZk-zdrQf-Rj

**Bayu Firmansyah Submission 1 MLT**

# **Data Loading**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

df = pd.read_csv('/content/drive/MyDrive/Restore/diabetes.csv')
df

"""# **Variable Description**"""

df.info()

df.describe().T

"""# **Univariate Analysis**

**Numerical Features**
"""

p = df.hist(figsize = (20,20))

"""# **Multivariate Analysis**

**Numerical Features**
"""

p = sns.pairplot(df,diag_kind = 'kde')

plt.figure(figsize=(12,10))
p=sns.heatmap(df.corr(), annot=True,cmap ='RdYlGn')

"""# **Data Preparation**"""

## presentase missing data pada setiap fitur
features = list(df.columns)
numoffeature = len(features)
columnscore={}

for i in range(0, numoffeature - 1):
  columnscore[features[i]] =  len(df[df[features[i]]== 0].index)/df[features[i]].shape[0]
columnscore

"""**Feature Selection**"""

target_dep = {}
for i in range(0, numoffeature - 1):
  target_dep[features[i]] = df[features[numoffeature - 1]].corr(df[features[i]])
avg_target_dep = sum(target_dep.values())/len(target_dep)

for i in range(0, numoffeature - 1):
  if(columnscore[features[i]] > 0.2 and target_dep[features[i]]):
    df.drop(features[i], inplace=True, axis=1)

features = list(df.columns)
numoffeature = len(features)
features

"""**Handle missing value**"""

## replace zeros with NaN
df1 = df.copy(deep = True)
cols = ["Glucose", "BloodPressure", "BMI"]
for col in cols:
    df1[col].replace(0,np.NaN,inplace=True)

## showing missing value
print(df1.isnull().sum())

p = df1.hist(figsize = (20,20))

## replace NaN values accordance their distribution
for col in cols:
  df1[col].fillna(df1[col].median(), inplace = True)

print(df1.isnull().sum())

df1.describe().T

"""**Handle Outlier**"""

cols1 = ["Pregnancies","Glucose", "BloodPressure", "BMI","DiabetesPedigreeFunction","Age"]
for col in cols1:
  plt.figure(figsize = (10, 5))
  sns.boxplot(df1[col]).set(title=col)

outliers1 = pd.DataFrame()

for col in "Pregnancies","Glucose", "BloodPressure", "BMI","DiabetesPedigreeFunction","Age":
  data = df1[col]
  Q1 = data.quantile(0.25)
  Q3 = data.quantile(0.75)
  IQR = Q3 - Q1

  threshold = 1.5
  outliers = df1[(data < Q1 - threshold * IQR) | (data > Q3 + threshold * IQR)]
  outliers1 = pd.concat([outliers, outliers1]).drop_duplicates()

df1=df1[~df1.isin(df1.loc[outliers1.index])].dropna()
df1

"""**Train Test Split**"""

from sklearn.model_selection import train_test_split

## data splitting
X = df1.drop(['Outcome'],axis = 1)
y = df1['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=12345)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""**Data standardization**"""

from sklearn.preprocessing import StandardScaler

## data standardization
scaler = StandardScaler()
scaler.fit(X_train)
X_trainsc = pd.DataFrame(scaler.transform(X_train),columns=["Pregnancies", "Glucose", "BloodPressure", "BMI", "DiabetesPedigreeFunction", "Age"])
X_trainsc.head()

X_testsc = pd.DataFrame(scaler.transform(X_test),columns=["Pregnancies", "Glucose", "BloodPressure", "BMI", "DiabetesPedigreeFunction", "Age"])
X_testsc.head()

"""# **Model Development**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score ,recall_score, f1_score, classification_report

"""**Random Forest**"""

RFC=RandomForestClassifier(n_estimators=100, max_depth=16, random_state=12345, n_jobs=-1)
RFC.fit(X_trainsc, y_train)
RFC_train_preds = RFC.predict(X_trainsc)
print(f"Training Accuracy Score: ", accuracy_score(y_train, RFC_train_preds))
print(f"\nClassification report:\n", classification_report(y_train, RFC_train_preds, digits=6))

"""**Decision Tree**"""

DTC=DecisionTreeClassifier(random_state=12345)
DTC.fit(X_trainsc, y_train)
DTC_train_preds = DTC.predict(X_trainsc)
print(f"Training Accuracy Score: ", accuracy_score(y_train, DTC_train_preds))
print(f"\nClassification report:\n", classification_report(y_train, DTC_train_preds, digits=6))

"""**K-Nearest Neighbor**"""

KNN=KNeighborsClassifier(n_neighbors=3)
KNN.fit(X_trainsc, y_train)
KNN_train_preds = KNN.predict(X_trainsc)
print(f"Training Accuracy Score: ", accuracy_score(y_train, KNN_train_preds))
print(f"\nClassification report:\n", classification_report(y_train, KNN_train_preds, digits=6))

"""# **Model Evaluation**

**Random Forest**
"""

RFC_test_preds = RFC.predict(X_testsc)
print(f"Testing Accuracy Score: ", accuracy_score(y_test, RFC_test_preds))
print(f"\nClassification report:\n", classification_report(y_test, RFC_test_preds, digits=6))

"""**Decision Tree**"""

DTC_test_preds = DTC.predict(X_testsc)
print(f"Testing Accuracy Score: ", accuracy_score(y_test, DTC_test_preds))
print(f"\nClassification report:\n", classification_report(y_test, DTC_test_preds, digits=6))

"""**K-Nearest Neighbor**"""

KNN_test_preds = KNN.predict(X_testsc)
print(f"Testing Accuracy Score: ", accuracy_score(y_test, KNN_test_preds))
print(f"\nClassification report:\n", classification_report(y_test, KNN_test_preds, digits=6))

eval = pd.DataFrame(columns=['Accuracy Score', 'F1 Score'], index=['Random Forest','Decision Tree','K-Nearest Neighbor'])
model_dict = {'Random Forest' : RFC, 'Decision Tree' : DTC, 'K-Nearest Neighbor' : KNN}
for name, model in model_dict.items():
    eval.loc[name, 'Accuracy Score'] = accuracy_score(y_true=y_test, y_pred=model.predict(X_testsc))
    eval.loc[name, 'F1 Score'] = f1_score(y_true=y_test, y_pred=model.predict(X_testsc))
eval

fig, ax = plt.subplots()
eval.sort_values(by='F1 Score', ascending=False).plot(kind='barh', ax=ax, zorder=5)
ax.grid(zorder=0)

"""**Kesimpulan**

Pada proyek ini disimpulkan bahwa model *Random Forest* merupakan model terbaik dengan nilai *accuracy score* dan *F1 Score* pada pengujian data training dan test tertinggi, model *Decision Tree* pada urutan kedua, dan *K-Nearest Neighbor* pada urutan terakhir.

Berdasarkan hasil proyek prediksi diabetes yang dilakukan dengan menggunakan algoritma *Random Forest*, *Decision Tree* dan *K-Nearest Neighbor*, proyek ini dapat membantu dalam deteksi dini penyakit diabetes dan memberikan prediksi yang akurat untuk pasien diabetes serta dapat digunakan sebagai dasar untuk pengembangan model prediksi yang lebih baik di masa depan.

Pengembangan selanjutnya dapat dilakukan dengan penggunaan hyperparameter tuning untuk meningkatkan kinerja model atau mencoba algoritma klasifikasi lainnya.
"""